{
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guia 02: Redes Neuronales con Keras - Tu Primera Red Profunda\n",
    "\n",
    "## Electiva II - Deep Learning | Tecnologico de Antioquia\n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo de aprendizaje:** Construir y entrenar redes neuronales multicapa usando TensorFlow/Keras, comprendiendo como la profundidad y la amplitud de la red afectan su capacidad de aprendizaje.\n",
    "\n",
    "**Conceptos nuevos en esta guia:**\n",
    "- Redes multicapa (MLP - Perceptron Multicapa)\n",
    "- Propagacion hacia adelante (Forward Propagation)\n",
    "- Retropropagacion del error (Backpropagation)\n",
    "- API Sequential y Functional de Keras\n",
    "- Compilacion y entrenamiento de modelos\n",
    "- Hiperparametros: capas, neuronas, epocas\n",
    "\n",
    "**Prerrequisito:** Guia 01 - Fundamentos de Deep Learning y el Perceptron\n",
    "\n",
    "**Duracion estimada:** 3 horas\n",
    "\n",
    "---\n",
    "\n",
    "> **IMPORTANTE:** Las secciones marcadas con **\\u270d\\ufe0f** requieren tu respuesta escrita. Estas respuestas son parte de tu evaluacion. No las dejes en blanco.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuracion del Entorno\n",
    "\n",
    "Primero configuremos todas las herramientas que necesitaremos. Si estas trabajando en **Google Colab**, TensorFlow ya viene preinstalado. Si estas en tu maquina local, necesitaras instalarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacion de TensorFlow (descomentar si es necesario)\n",
    "# En Google Colab normalmente ya esta instalado\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Importacion de librerias\n",
    "# ============================================================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuracion de estilo para graficas\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Librerias importadas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Verificacion de la version de TensorFlow\n",
    "# ============================================================\n",
    "print(f\"Version de TensorFlow: {tf.__version__}\")\n",
    "print(f\"Version de Keras: {keras.__version__}\")\n",
    "print(f\"Version de NumPy: {np.__version__}\")\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nGPU disponible: {gpus}\")\n",
    "    print(\"El entrenamiento sera mas rapido con GPU.\")\n",
    "else:\n",
    "    print(\"\\nNo se detecto GPU. Se usara CPU.\")\n",
    "    print(\"Esto es normal si estas en tu computador sin GPU dedicada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Semilla de reproducibilidad\n",
    "# ============================================================\n",
    "# Fijamos una semilla para que todos obtengamos resultados similares.\n",
    "# Esto es importante en ciencia de datos: necesitamos que los\n",
    "# experimentos sean reproducibles.\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(f\"Semilla de reproducibilidad fijada en: {SEED}\")\n",
    "print(\"Esto garantiza que los resultados sean consistentes entre ejecuciones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Marco Teorico\n",
    "\n",
    "### 2.1 Del Perceptron a la Red Multicapa: La Solucion al Problema XOR\n",
    "\n",
    "En la **Guia 01** construimos un perceptron simple y descubrimos su gran limitacion: **no puede resolver problemas que no son linealmente separables**, como el problema XOR.\n",
    "\n",
    "Recordemos el problema XOR:\n",
    "\n",
    "| Entrada 1 | Entrada 2 | Salida XOR |\n",
    "|:---------:|:---------:|:----------:|\n",
    "| 0         | 0         | 0          |\n",
    "| 0         | 1         | 1          |\n",
    "| 1         | 0         | 1          |\n",
    "| 1         | 1         | 0          |\n",
    "\n",
    "Un solo perceptron no puede trazar una unica linea recta que separe los 0s de los 1s en este caso. **La solucion** fue propuesta ya en los anos 80: usar **multiples perceptrones organizados en capas**, creando lo que llamamos una **Red Neuronal Multicapa** o **MLP** (Multi-Layer Perceptron).\n",
    "\n",
    "La idea es simple pero poderosa:\n",
    "- **Capa 1 (oculta):** Dos neuronas que aprenden dos fronteras lineales distintas.\n",
    "- **Capa 2 (salida):** Una neurona que combina las dos fronteras para resolver XOR.\n",
    "\n",
    "```\n",
    "Ejemplo de red para XOR:\n",
    "\n",
    "  Entradas      Capa Oculta      Salida\n",
    "                 --------\n",
    "  x1 ---------> | h1     | ------\\\n",
    "       \\   /    --------          --------\n",
    "        \\ /                 ---> |  y     |\n",
    "        / \\                 /     --------\n",
    "       /   \\    --------   /\n",
    "  x2 ---------> | h2     | ------/\n",
    "                 --------\n",
    "```\n",
    "\n",
    "Cada conexion (flecha) tiene un **peso** que se ajusta durante el entrenamiento. La capa oculta transforma los datos en una nueva representacion donde el problema **si es linealmente separable**.\n",
    "\n",
    "### 2.2 Arquitectura de una Red Neuronal\n",
    "\n",
    "Una red neuronal se compone de tres tipos de capas:\n",
    "\n",
    "```\n",
    "  CAPA DE           CAPAS              CAPA DE\n",
    "  ENTRADA          OCULTAS             SALIDA\n",
    "  -------     -------  -------        -------\n",
    "  | x1  | --> | h1  |  | h4  |  ----> | y1  |\n",
    "  -------     -------  -------        -------\n",
    "  -------     -------  -------        -------\n",
    "  | x2  | --> | h2  |  | h5  |  ----> | y2  |\n",
    "  -------     -------  -------        -------\n",
    "  -------     -------  -------        -------\n",
    "  | x3  | --> | h3  |  | h6  |  ----> | y3  |\n",
    "  -------     -------  -------        -------\n",
    "\n",
    "  (3 neuronas)  (3)      (3)          (3 neuronas)\n",
    "```\n",
    "\n",
    "1. **Capa de entrada:** Recibe los datos crudos. No hace ningun calculo. El numero de neuronas es igual al numero de caracteristicas de los datos. Para una imagen de 28x28 pixeles, tendriamos 784 neuronas de entrada.\n",
    "\n",
    "2. **Capas ocultas:** Aqui es donde sucede la \"magia\". Cada capa oculta transforma la representacion de los datos. Las primeras capas aprenden patrones simples (bordes, texturas) y las capas mas profundas aprenden patrones mas complejos (formas, objetos). Podemos tener 1, 2, 10 o cientos de capas ocultas.\n",
    "\n",
    "3. **Capa de salida:** Produce la prediccion final. Para clasificacion de 10 digitos, tendremos 10 neuronas (una por cada digito del 0 al 9).\n",
    "\n",
    "**Terminologia importante:**\n",
    "- **Red densa (fully connected):** cada neurona de una capa esta conectada a TODAS las neuronas de la capa siguiente.\n",
    "- **Profundidad:** numero de capas ocultas. Mas capas = red mas \"profunda\" (de ahi el nombre \"Deep Learning\").\n",
    "- **Amplitud:** numero de neuronas por capa.\n",
    "\n",
    "### 2.3 Propagacion Hacia Adelante (Forward Propagation)\n",
    "\n",
    "Es el proceso mediante el cual los datos de entrada viajan a traves de la red, capa por capa, hasta producir una salida. En cada neurona ocurre lo siguiente:\n",
    "\n",
    "**Paso 1 - Combinacion lineal:**\n",
    "\n",
    "$$z = \\sum_{i=1}^{n} w_i \\cdot x_i + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "Donde:\n",
    "- $x_i$ son las entradas (o las salidas de la capa anterior)\n",
    "- $w_i$ son los pesos de las conexiones\n",
    "- $b$ es el sesgo (bias)\n",
    "- $z$ es la preactivacion\n",
    "\n",
    "**Paso 2 - Funcion de activacion:**\n",
    "\n",
    "$$a = f(z)$$\n",
    "\n",
    "Donde $f$ es una funcion de activacion no lineal. Las mas comunes son:\n",
    "\n",
    "- **ReLU** (Rectified Linear Unit): $f(z) = \\max(0, z)$ -- la mas usada en capas ocultas\n",
    "- **Sigmoid:** $f(z) = \\frac{1}{1 + e^{-z}}$ -- usada en salida para clasificacion binaria\n",
    "- **Softmax:** $f(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}$ -- usada en salida para clasificacion multiclase\n",
    "\n",
    "**Ejemplo concreto de forward propagation:**\n",
    "\n",
    "Imaginemos una red con 2 entradas, 1 capa oculta de 3 neuronas y 1 salida:\n",
    "\n",
    "```\n",
    "Entrada: x = [0.5, 0.8]\n",
    "\n",
    "Capa oculta (3 neuronas):\n",
    "  h1 = ReLU(w11*0.5 + w12*0.8 + b1)\n",
    "  h2 = ReLU(w21*0.5 + w22*0.8 + b2)\n",
    "  h3 = ReLU(w31*0.5 + w32*0.8 + b3)\n",
    "\n",
    "Capa de salida (1 neurona):\n",
    "  y = Sigmoid(w1*h1 + w2*h2 + w3*h3 + b)\n",
    "```\n",
    "\n",
    "Esto se puede expresar de forma matricial para toda una capa:\n",
    "\n",
    "$$\\mathbf{a}^{[l]} = f\\left(\\mathbf{W}^{[l]} \\cdot \\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}\\right)$$\n",
    "\n",
    "Donde $l$ indica el numero de capa.\n",
    "\n",
    "### 2.4 Backpropagation y Descenso de Gradiente\n",
    "\n",
    "Una vez que la red produce una prediccion (forward propagation), necesitamos saber **que tan equivocada esta** y **como corregirla**. Aqui entran dos conceptos fundamentales:\n",
    "\n",
    "#### Funcion de perdida (Loss Function)\n",
    "\n",
    "Mide la diferencia entre la prediccion de la red ($\\hat{y}$) y el valor real ($y$).\n",
    "\n",
    "**Para clasificacion multiclase** usamos **Categorical Cross-Entropy**:\n",
    "\n",
    "$$L = -\\sum_{i=1}^{C} y_i \\cdot \\log(\\hat{y}_i)$$\n",
    "\n",
    "Donde $C$ es el numero de clases, $y_i$ es 1 si la clase $i$ es la correcta y 0 en caso contrario, y $\\hat{y}_i$ es la probabilidad predicha para la clase $i$.\n",
    "\n",
    "**Para regresion** usamos **Error Cuadratico Medio (MSE)**:\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "#### Backpropagation (Retropropagacion del error)\n",
    "\n",
    "Es el algoritmo que calcula **como cada peso contribuye al error**. Funciona usando la **regla de la cadena** del calculo diferencial, propagando el error desde la salida hacia las capas anteriores.\n",
    "\n",
    "Intuitivamente, es como preguntar: \"Si cambio ligeramente este peso, ?cuanto cambia el error total?\" Eso es el **gradiente**:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}^{[l]}}$$\n",
    "\n",
    "El gradiente nos dice la **direccion** y **magnitud** del cambio necesario.\n",
    "\n",
    "#### Descenso de Gradiente\n",
    "\n",
    "Una vez calculados los gradientes, actualizamos cada peso en la direccion que reduce el error:\n",
    "\n",
    "$$w_{nuevo} = w_{actual} - \\alpha \\cdot \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "Donde $\\alpha$ es la **tasa de aprendizaje** (learning rate), un hiperparametro que controla que tan grandes son los pasos de actualizacion.\n",
    "\n",
    "**Analogia:** Imagina que estas en una montana con niebla y quieres llegar al punto mas bajo (valle). No puedes ver el valle, pero puedes sentir la inclinacion del terreno bajo tus pies. El descenso de gradiente es como dar pasos pequenos siempre en la direccion donde el terreno baja mas.\n",
    "\n",
    "- $\\alpha$ muy grande: das saltos enormes y puedes saltar por encima del valle.\n",
    "- $\\alpha$ muy pequeno: avanzas tan lento que nunca llegas.\n",
    "- $\\alpha$ adecuado: llegas al valle de forma eficiente.\n",
    "\n",
    "**Optimizador Adam:** En la practica, usamos optimizadores mas sofisticados que el descenso de gradiente basico. **Adam** (Adaptive Moment Estimation) adapta la tasa de aprendizaje para cada peso individualmente, lo que generalmente funciona muy bien sin necesidad de ajustar mucho.\n",
    "\n",
    "### 2.5 Introduccion a Keras\n",
    "\n",
    "**Keras** es una API de alto nivel para construir y entrenar redes neuronales. Originalmente era una libreria independiente, pero ahora esta integrada en **TensorFlow** como `tf.keras`.\n",
    "\n",
    "**Ventajas de Keras:**\n",
    "- Sintaxis simple e intuitiva\n",
    "- Permite prototipar rapidamente\n",
    "- Soporta CPU y GPU sin cambiar codigo\n",
    "- Gran comunidad y documentacion\n",
    "\n",
    "**Dos formas de construir modelos:**\n",
    "\n",
    "| Caracteristica | API Sequential | API Functional |\n",
    "|:--------------|:--------------|:---------------|\n",
    "| Complejidad | Simple | Mas flexible |\n",
    "| Tipo de modelos | Lineales (capa tras capa) | Cualquier arquitectura |\n",
    "| Multiples entradas/salidas | No | Si |\n",
    "| Conexiones residuales | No | Si |\n",
    "| Recomendado para | Principiantes, redes simples | Arquitecturas complejas |\n",
    "\n",
    "**Flujo tipico en Keras:**\n",
    "1. **Definir** la arquitectura (capas, neuronas, activaciones)\n",
    "2. **Compilar** el modelo (optimizador, funcion de perdida, metricas)\n",
    "3. **Entrenar** el modelo con datos (`.fit()`)\n",
    "4. **Evaluar** el modelo con datos de prueba (`.evaluate()`)\n",
    "5. **Predecir** con datos nuevos (`.predict()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Carga y Exploracion de Datos: MNIST\n",
    "\n",
    "**MNIST** (Modified National Institute of Standards and Technology) es uno de los datasets mas clasicos en Machine Learning. Contiene **70,000 imagenes** de digitos escritos a mano (0-9), cada una de **28x28 pixeles** en escala de grises.\n",
    "\n",
    "- **60,000** imagenes de entrenamiento\n",
    "- **10,000** imagenes de prueba\n",
    "- **10 clases** (digitos del 0 al 9)\n",
    "\n",
    "Es el equivalente al \"Hola Mundo\" del Deep Learning. Si tu red puede clasificar estos digitos, has dado un gran paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.1 Cargar el dataset MNIST\n",
    "# ============================================================\n",
    "# Keras tiene MNIST integrado, lo cual facilita mucho la carga.\n",
    "# Los datos ya vienen divididos en entrenamiento y prueba.\n",
    "\n",
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Dataset MNIST cargado exitosamente!\")\n",
    "print(f\"\\n--- Conjunto de entrenamiento ---\")\n",
    "print(f\"  Imagenes (x_train): {x_train_raw.shape}\")\n",
    "print(f\"  Etiquetas (y_train): {y_train_raw.shape}\")\n",
    "print(f\"\\n--- Conjunto de prueba ---\")\n",
    "print(f\"  Imagenes (x_test): {x_test_raw.shape}\")\n",
    "print(f\"  Etiquetas (y_test): {y_test_raw.shape}\")\n",
    "print(f\"\\n--- Informacion adicional ---\")\n",
    "print(f\"  Tipo de datos de las imagenes: {x_train_raw.dtype}\")\n",
    "print(f\"  Valor minimo de un pixel: {x_train_raw.min()}\")\n",
    "print(f\"  Valor maximo de un pixel: {x_train_raw.max()}\")\n",
    "print(f\"  Clases unicas: {np.unique(y_train_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.2 Visualizar ejemplos del dataset\n",
    "# ============================================================\n",
    "# Siempre es buena practica visualizar los datos antes de\n",
    "# construir cualquier modelo. Veamos una cuadricula de ejemplos.\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "fig.suptitle('Ejemplos del Dataset MNIST', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Seleccionamos un indice aleatorio\n",
    "    idx = np.random.randint(0, len(x_train_raw))\n",
    "    ax.imshow(x_train_raw[idx], cmap='gray')\n",
    "    ax.set_title(f'Etiqueta: {y_train_raw[idx]}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.3 Distribucion de clases\n",
    "# ============================================================\n",
    "# Verifiquemos que las clases esten balanceadas\n",
    "# (que haya una cantidad similar de cada digito).\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribucion en entrenamiento\n",
    "clases_train, conteos_train = np.unique(y_train_raw, return_counts=True)\n",
    "axes[0].bar(clases_train, conteos_train, color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Distribucion de clases - Entrenamiento', fontsize=14)\n",
    "axes[0].set_xlabel('Digito')\n",
    "axes[0].set_ylabel('Cantidad de imagenes')\n",
    "axes[0].set_xticks(range(10))\n",
    "\n",
    "# Distribucion en prueba\n",
    "clases_test, conteos_test = np.unique(y_test_raw, return_counts=True)\n",
    "axes[1].bar(clases_test, conteos_test, color='coral', edgecolor='black')\n",
    "axes[1].set_title('Distribucion de clases - Prueba', fontsize=14)\n",
    "axes[1].set_xlabel('Digito')\n",
    "axes[1].set_ylabel('Cantidad de imagenes')\n",
    "axes[1].set_xticks(range(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Las clases estan razonablemente balanceadas.\")\n",
    "print(f\"Rango de imagenes por clase (train): {conteos_train.min()} - {conteos_train.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Preprocesamiento de los datos\n",
    "\n",
    "Antes de alimentar los datos a la red neuronal, necesitamos realizar dos transformaciones fundamentales:\n",
    "\n",
    "**1. Normalizacion:** Los valores de los pixeles van de 0 a 255. Dividimos entre 255 para llevarlos al rango [0, 1]. Esto ayuda a que:\n",
    "- Los gradientes sean mas estables durante el entrenamiento\n",
    "- El optimizador converja mas rapido\n",
    "- Las funciones de activacion trabajen en su rango optimo\n",
    "\n",
    "**2. Reshape (aplanamiento):** Las imagenes son matrices de 28x28. Pero una red densa (fully connected) espera un vector unidimensional como entrada. Asi que \"aplanaamos\" cada imagen de 28x28 a un vector de 784 elementos.\n",
    "\n",
    "**3. One-hot encoding:** Las etiquetas son numeros (0-9). Para clasificacion multiclase con softmax, necesitamos convertirlas a vectores one-hot. Por ejemplo:\n",
    "- Digito 3 --> [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "- Digito 7 --> [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3.4 Preprocesamiento\n",
    "# ============================================================\n",
    "\n",
    "# --- Paso 1: Normalizacion ---\n",
    "# Convertimos de uint8 (0-255) a float32 (0.0-1.0)\n",
    "x_train_norm = x_train_raw.astype('float32') / 255.0\n",
    "x_test_norm = x_test_raw.astype('float32') / 255.0\n",
    "\n",
    "print(\"--- Despues de la normalizacion ---\")\n",
    "print(f\"Tipo de dato: {x_train_norm.dtype}\")\n",
    "print(f\"Valor minimo: {x_train_norm.min()}\")\n",
    "print(f\"Valor maximo: {x_train_norm.max()}\")\n",
    "\n",
    "# --- Paso 2: Reshape (aplanar de 28x28 a 784) ---\n",
    "x_train = x_train_norm.reshape(-1, 784)  # -1 significa \"calcula esta dimension automaticamente\"\n",
    "x_test = x_test_norm.reshape(-1, 784)\n",
    "\n",
    "print(f\"\\n--- Despues del reshape ---\")\n",
    "print(f\"Forma de x_train: {x_train.shape}  (60000 imagenes, cada una con 784 pixeles)\")\n",
    "print(f\"Forma de x_test: {x_test.shape}\")\n",
    "\n",
    "# --- Paso 3: One-hot encoding de las etiquetas ---\n",
    "num_clases = 10\n",
    "y_train = to_categorical(y_train_raw, num_clases)\n",
    "y_test = to_categorical(y_test_raw, num_clases)\n",
    "\n",
    "print(f\"\\n--- Despues del one-hot encoding ---\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "print(f\"Forma de y_test: {y_test.shape}\")\n",
    "print(f\"\\nEjemplo - Digito {y_train_raw[0]}:\")\n",
    "print(f\"  Etiqueta original: {y_train_raw[0]}\")\n",
    "print(f\"  One-hot encoding:  {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Experimentacion Guiada\n",
    "\n",
    "Ahora viene la parte mas emocionante. Vamos a construir redes neuronales reales usando Keras y experimentar con diferentes configuraciones para entender como afectan al rendimiento.\n",
    "\n",
    "---\n",
    "\n",
    "### Experimento 1: Tu Primera Red Neuronal con Keras (Sequential API)\n",
    "\n",
    "Vamos a construir paso a paso tu primera red neuronal. Sera una red simple pero sorprendentemente efectiva:\n",
    "\n",
    "```\n",
    "  Entrada (784)  -->  Capa Oculta (128, ReLU)  -->  Salida (10, Softmax)\n",
    "```\n",
    "\n",
    "**Arquitectura:**\n",
    "- **Entrada:** 784 neuronas (los 784 pixeles de cada imagen)\n",
    "- **Capa oculta:** 128 neuronas con activacion ReLU\n",
    "- **Salida:** 10 neuronas con activacion Softmax (una probabilidad por cada digito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 1: Primera red neuronal con Keras\n",
    "# ============================================================\n",
    "\n",
    "# --- Paso 1: Definir la arquitectura ---\n",
    "# Usamos la API Sequential: las capas se apilan una tras otra.\n",
    "\n",
    "modelo_1 = keras.Sequential([\n",
    "    # Capa oculta: 128 neuronas, activacion ReLU\n",
    "    # input_shape=(784,) le dice a Keras el tamano de la entrada\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,), name='capa_oculta'),\n",
    "    \n",
    "    # Capa de salida: 10 neuronas (una por clase), activacion Softmax\n",
    "    # Softmax convierte las salidas en probabilidades que suman 1\n",
    "    layers.Dense(10, activation='softmax', name='capa_salida')\n",
    "], name='Mi_Primera_Red')\n",
    "\n",
    "print(\"Modelo creado exitosamente!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 2: Ver el resumen del modelo ---\n",
    "# model.summary() nos muestra:\n",
    "# - Nombre de cada capa\n",
    "# - Forma de salida de cada capa\n",
    "# - Numero de parametros (pesos + sesgos) por capa\n",
    "\n",
    "modelo_1.summary()\n",
    "\n",
    "# Desglose de parametros:\n",
    "# Capa oculta: 784 entradas * 128 neuronas + 128 sesgos = 100,480 parametros\n",
    "# Capa salida: 128 entradas * 10 neuronas + 10 sesgos = 1,290 parametros\n",
    "# Total: 101,770 parametros\n",
    "print(f\"\\nTotal de parametros entrenables: {modelo_1.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 3: Compilar el modelo ---\n",
    "# Compilar = configurar COMO el modelo va a aprender.\n",
    "# Necesitamos especificar:\n",
    "#   - optimizer: algoritmo de optimizacion (como actualizar los pesos)\n",
    "#   - loss: funcion de perdida (como medir el error)\n",
    "#   - metrics: metricas adicionales para monitorear\n",
    "\n",
    "modelo_1.compile(\n",
    "    optimizer='adam',                       # Optimizador Adam (adaptativo, buena opcion general)\n",
    "    loss='categorical_crossentropy',        # Funcion de perdida para clasificacion multiclase\n",
    "    metrics=['accuracy']                    # Queremos ver la precision durante el entrenamiento\n",
    ")\n",
    "\n",
    "print(\"Modelo compilado exitosamente!\")\n",
    "print(\"  Optimizador: Adam\")\n",
    "print(\"  Funcion de perdida: Categorical Cross-Entropy\")\n",
    "print(\"  Metrica: Accuracy (Precision)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 4: Entrenar el modelo ---\n",
    "# model.fit() entrena la red. Los parametros principales son:\n",
    "#   - x, y: datos de entrada y etiquetas\n",
    "#   - epochs: cuantas veces recorrer todo el dataset\n",
    "#   - batch_size: cuantas imagenes procesar a la vez\n",
    "#   - validation_split: porcentaje de datos para validacion\n",
    "#   - verbose: nivel de detalle en la salida (1 = barra de progreso)\n",
    "\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "historial_1 = modelo_1.fit(\n",
    "    x_train, y_train,           # Datos de entrenamiento\n",
    "    epochs=10,                   # 10 pasadas completas por el dataset\n",
    "    batch_size=128,              # Procesar 128 imagenes por paso\n",
    "    validation_split=0.2,        # Usar 20% para validacion (12,000 imagenes)\n",
    "    verbose=1                    # Mostrar progreso\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 5: Graficar las curvas de entrenamiento ---\n",
    "# Estas curvas son FUNDAMENTALES para diagnosticar el comportamiento del modelo.\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Grafica de Accuracy\n",
    "axes[0].plot(historial_1.history['accuracy'], label='Entrenamiento', linewidth=2)\n",
    "axes[0].plot(historial_1.history['val_accuracy'], label='Validacion', linewidth=2, linestyle='--')\n",
    "axes[0].set_title('Accuracy (Precision) por Epoca', fontsize=14)\n",
    "axes[0].set_xlabel('Epoca')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Grafica de Loss\n",
    "axes[1].plot(historial_1.history['loss'], label='Entrenamiento', linewidth=2)\n",
    "axes[1].plot(historial_1.history['val_loss'], label='Validacion', linewidth=2, linestyle='--')\n",
    "axes[1].set_title('Loss (Perdida) por Epoca', fontsize=14)\n",
    "axes[1].set_xlabel('Epoca')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 1: Curvas de Entrenamiento', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 6: Evaluar en el conjunto de prueba ---\n",
    "# IMPORTANTE: El conjunto de prueba NUNCA se usa durante el entrenamiento.\n",
    "# Es la medida final y objetiva del rendimiento del modelo.\n",
    "\n",
    "loss_test, accuracy_test = modelo_1.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESULTADOS EN EL CONJUNTO DE PRUEBA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Loss (perdida):     {loss_test:.4f}\")\n",
    "print(f\"  Accuracy (precision): {accuracy_test:.4f}  ({accuracy_test*100:.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTu primera red neuronal clasifica correctamente el {accuracy_test*100:.2f}% de los digitos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta - Experimento 1:\n",
    "\n",
    "**Pregunta:** ?Que accuracy obtuviste en el set de prueba? ?Como se comparan las curvas de entrenamiento vs validacion? ?Hay indicios de overfitting?\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experimento 2: Efecto del Numero de Capas Ocultas\n",
    "\n",
    "Ahora vamos a investigar una pregunta clave: **?agregar mas capas mejora el modelo?**\n",
    "\n",
    "Compararemos tres arquitecturas:\n",
    "\n",
    "| Modelo | Capas Ocultas | Neuronas |\n",
    "|:------:|:-------------:|:--------:|\n",
    "| A | 1 capa | 128 |\n",
    "| B | 2 capas | 128, 64 |\n",
    "| C | 3 capas | 256, 128, 64 |\n",
    "\n",
    "Para hacer una comparacion justa, usaremos los mismos hiperparametros (optimizador, epochs, batch_size) en los tres modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 2: Efecto del numero de capas ocultas\n",
    "# ============================================================\n",
    "\n",
    "# Funcion auxiliar para crear, compilar, entrenar y evaluar un modelo.\n",
    "# Esto evita repetir codigo y nos permite experimentar facilmente.\n",
    "\n",
    "def crear_y_entrenar_modelo(capas_ocultas, nombre, epochs=10, verbose=0):\n",
    "    \"\"\"\n",
    "    Crea, compila, entrena y evalua un modelo con la configuracion dada.\n",
    "    \n",
    "    Parametros:\n",
    "    - capas_ocultas: lista de tuplas (neuronas, activacion) para cada capa oculta\n",
    "    - nombre: nombre del modelo\n",
    "    - epochs: numero de epocas de entrenamiento\n",
    "    - verbose: nivel de detalle de la salida\n",
    "    \n",
    "    Retorna:\n",
    "    - modelo entrenado, historial de entrenamiento, tiempo de entrenamiento\n",
    "    \"\"\"\n",
    "    # Crear modelo\n",
    "    modelo = keras.Sequential(name=nombre)\n",
    "    \n",
    "    # Primera capa oculta (necesita input_shape)\n",
    "    modelo.add(layers.Dense(capas_ocultas[0], activation='relu', input_shape=(784,)))\n",
    "    \n",
    "    # Capas ocultas adicionales\n",
    "    for neuronas in capas_ocultas[1:]:\n",
    "        modelo.add(layers.Dense(neuronas, activation='relu'))\n",
    "    \n",
    "    # Capa de salida\n",
    "    modelo.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compilar\n",
    "    modelo.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Entrenar y medir tiempo\n",
    "    inicio = time.time()\n",
    "    historial = modelo.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    tiempo = time.time() - inicio\n",
    "    \n",
    "    return modelo, historial, tiempo\n",
    "\n",
    "print(\"Funcion auxiliar definida correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar los tres modelos\n",
    "print(\"Entrenando Modelo A (1 capa oculta: 128)...\")\n",
    "modelo_A, hist_A, tiempo_A = crear_y_entrenar_modelo([128], 'Modelo_A_1capa')\n",
    "print(f\"  Completado en {tiempo_A:.1f}s\")\n",
    "\n",
    "print(\"\\nEntrenando Modelo B (2 capas ocultas: 128, 64)...\")\n",
    "modelo_B, hist_B, tiempo_B = crear_y_entrenar_modelo([128, 64], 'Modelo_B_2capas')\n",
    "print(f\"  Completado en {tiempo_B:.1f}s\")\n",
    "\n",
    "print(\"\\nEntrenando Modelo C (3 capas ocultas: 256, 128, 64)...\")\n",
    "modelo_C, hist_C, tiempo_C = crear_y_entrenar_modelo([256, 128, 64], 'Modelo_C_3capas')\n",
    "print(f\"  Completado en {tiempo_C:.1f}s\")\n",
    "\n",
    "print(\"\\nTodos los modelos entrenados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar los tres modelos en el conjunto de prueba\n",
    "loss_A, acc_A = modelo_A.evaluate(x_test, y_test, verbose=0)\n",
    "loss_B, acc_B = modelo_B.evaluate(x_test, y_test, verbose=0)\n",
    "loss_C, acc_C = modelo_C.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Mostrar tabla comparativa\n",
    "print(\"=\"*75)\n",
    "print(\"COMPARACION DE MODELOS - Experimento 2\")\n",
    "print(\"=\"*75)\n",
    "print(f\"{'Modelo':<25} {'Capas':<15} {'Params':<12} {'Accuracy':<12} {'Loss':<10} {'Tiempo':<10}\")\n",
    "print(\"-\"*75)\n",
    "print(f\"{'A (1 capa: 128)':<25} {'1':<15} {modelo_A.count_params():<12,} {acc_A:<12.4f} {loss_A:<10.4f} {tiempo_A:<10.1f}s\")\n",
    "print(f\"{'B (2 capas: 128,64)':<25} {'2':<15} {modelo_B.count_params():<12,} {acc_B:<12.4f} {loss_B:<10.4f} {tiempo_B:<10.1f}s\")\n",
    "print(f\"{'C (3 capas: 256,128,64)':<25} {'3':<15} {modelo_C.count_params():<12,} {acc_C:<12.4f} {loss_C:<10.4f} {tiempo_C:<10.1f}s\")\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las curvas de entrenamiento de los tres modelos superpuestas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colores = ['#2196F3', '#FF9800', '#4CAF50']  # Azul, Naranja, Verde\n",
    "nombres = ['A (1 capa)', 'B (2 capas)', 'C (3 capas)']\n",
    "historiales = [hist_A, hist_B, hist_C]\n",
    "\n",
    "for i, (hist, nombre, color) in enumerate(zip(historiales, nombres, colores)):\n",
    "    # Accuracy\n",
    "    axes[0].plot(hist.history['accuracy'], label=f'{nombre} - Train', color=color, linewidth=2)\n",
    "    axes[0].plot(hist.history['val_accuracy'], label=f'{nombre} - Val', color=color, linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Loss\n",
    "    axes[1].plot(hist.history['loss'], label=f'{nombre} - Train', color=color, linewidth=2)\n",
    "    axes[1].plot(hist.history['val_loss'], label=f'{nombre} - Val', color=color, linewidth=2, linestyle='--')\n",
    "\n",
    "axes[0].set_title('Accuracy por Epoca', fontsize=14)\n",
    "axes[0].set_xlabel('Epoca')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Loss por Epoca', fontsize=14)\n",
    "axes[1].set_xlabel('Epoca')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Experimento 2: Comparacion por Numero de Capas', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta - Experimento 2:\n",
    "\n",
    "**Pregunta:** ?Agregar mas capas siempre mejora el resultado? ?Que observaste con el tiempo de entrenamiento? ?Cual modelo tiene mejor relacion rendimiento/complejidad?\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experimento 3: Efecto del Numero de Neuronas por Capa\n",
    "\n",
    "Ahora mantendremos fija la estructura (2 capas ocultas) y variaremos la **cantidad de neuronas**. Esto nos permitira entender la relacion entre la amplitud de la red y su capacidad de aprendizaje.\n",
    "\n",
    "| Modelo | Capa 1 | Capa 2 | Total aproximado de parametros |\n",
    "|:------:|:------:|:------:|:------------------------------:|\n",
    "| Pequeno | 32 | 16 | ~25,000 |\n",
    "| Mediano | 64 | 32 | ~52,000 |\n",
    "| Grande | 128 | 64 | ~110,000 |\n",
    "| Muy Grande | 512 | 256 | ~530,000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 3: Efecto del numero de neuronas por capa\n",
    "# ============================================================\n",
    "\n",
    "configuraciones = [\n",
    "    ([32, 16], 'Pequeno_32_16'),\n",
    "    ([64, 32], 'Mediano_64_32'),\n",
    "    ([128, 64], 'Grande_128_64'),\n",
    "    ([512, 256], 'MuyGrande_512_256')\n",
    "]\n",
    "\n",
    "resultados_exp3 = []\n",
    "\n",
    "for config, nombre in configuraciones:\n",
    "    print(f\"Entrenando modelo {nombre}...\")\n",
    "    modelo, hist, tiempo = crear_y_entrenar_modelo(config, nombre)\n",
    "    loss, acc = modelo.evaluate(x_test, y_test, verbose=0)\n",
    "    params = modelo.count_params()\n",
    "    resultados_exp3.append({\n",
    "        'nombre': nombre,\n",
    "        'config': config,\n",
    "        'accuracy': acc,\n",
    "        'loss': loss,\n",
    "        'params': params,\n",
    "        'tiempo': tiempo,\n",
    "        'historial': hist\n",
    "    })\n",
    "    print(f\"  Accuracy: {acc:.4f} | Params: {params:,} | Tiempo: {tiempo:.1f}s\")\n",
    "\n",
    "print(\"\\nTodos los modelos entrenados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa del Experimento 3\n",
    "print(\"=\"*80)\n",
    "print(\"COMPARACION DE MODELOS - Experimento 3: Numero de Neuronas\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Modelo':<25} {'Neuronas':<15} {'Parametros':<15} {'Accuracy':<12} {'Tiempo':<10}\")\n",
    "print(\"-\"*80)\n",
    "for r in resultados_exp3:\n",
    "    print(f\"{r['nombre']:<25} {str(r['config']):<15} {r['params']:<15,} {r['accuracy']:<12.4f} {r['tiempo']:<10.1f}s\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizacion: Accuracy vs Numero de parametros\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "params_list = [r['params'] for r in resultados_exp3]\n",
    "acc_list = [r['accuracy'] for r in resultados_exp3]\n",
    "tiempo_list = [r['tiempo'] for r in resultados_exp3]\n",
    "nombres_list = [r['nombre'].replace('_', '\\n') for r in resultados_exp3]\n",
    "\n",
    "# Accuracy vs Parametros\n",
    "axes[0].bar(nombres_list, acc_list, color=['#E3F2FD', '#90CAF9', '#42A5F5', '#1565C0'], edgecolor='black')\n",
    "axes[0].set_title('Accuracy vs Configuracion', fontsize=14)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_ylim(0.95, 1.0)\n",
    "for i, (nombre, acc) in enumerate(zip(nombres_list, acc_list)):\n",
    "    axes[0].text(i, acc + 0.001, f'{acc:.4f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Parametros vs Tiempo\n",
    "axes[1].bar(nombres_list, params_list, color=['#E8F5E9', '#A5D6A7', '#66BB6A', '#2E7D32'], edgecolor='black')\n",
    "axes[1].set_title('Numero de Parametros por Modelo', fontsize=14)\n",
    "axes[1].set_ylabel('Parametros')\n",
    "for i, (nombre, p) in enumerate(zip(nombres_list, params_list)):\n",
    "    axes[1].text(i, p + 5000, f'{p:,}', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Experimento 3: Efecto del Numero de Neuronas', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta - Experimento 3:\n",
    "\n",
    "**Pregunta:** ?Cual es la relacion entre el numero de neuronas y el rendimiento? ?Y con el numero de parametros? (Observa los valores de `model.count_params()` en la tabla). ?Tiene sentido usar siempre el modelo mas grande?\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experimento 4: Efecto del Numero de Epocas\n",
    "\n",
    "Una **epoca** es una pasada completa por todo el dataset de entrenamiento. Mas epocas significa que el modelo ve los datos mas veces. Pero, ?mas siempre es mejor?\n",
    "\n",
    "Vamos a entrenar el mismo modelo con diferentes cantidades de epocas y observar cuando el modelo **deja de mejorar** o incluso empieza a **empeorar** (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 4: Efecto del numero de epocas\n",
    "# ============================================================\n",
    "\n",
    "# Entrenaremos un unico modelo por 50 epocas y analizaremos\n",
    "# el comportamiento en distintos puntos.\n",
    "\n",
    "print(\"Entrenando modelo con 50 epocas (esto puede tardar un poco)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "modelo_epocas = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "], name='Modelo_Epocas')\n",
    "\n",
    "modelo_epocas.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "historial_epocas = modelo_epocas.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar el comportamiento a lo largo de las 50 epocas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "epocas = range(1, 51)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epocas, historial_epocas.history['accuracy'], label='Entrenamiento', linewidth=2, color='#2196F3')\n",
    "axes[0].plot(epocas, historial_epocas.history['val_accuracy'], label='Validacion', linewidth=2, color='#FF5722', linestyle='--')\n",
    "axes[0].set_title('Accuracy vs Epocas', fontsize=14)\n",
    "axes[0].set_xlabel('Epoca')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Marcar puntos de referencia en las epocas 5, 10, 20\n",
    "for ep in [5, 10, 20]:\n",
    "    axes[0].axvline(x=ep, color='gray', linestyle=':', alpha=0.5)\n",
    "    axes[0].text(ep, axes[0].get_ylim()[0] + 0.001, f'Ep {ep}', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epocas, historial_epocas.history['loss'], label='Entrenamiento', linewidth=2, color='#2196F3')\n",
    "axes[1].plot(epocas, historial_epocas.history['val_loss'], label='Validacion', linewidth=2, color='#FF5722', linestyle='--')\n",
    "axes[1].set_title('Loss vs Epocas', fontsize=14)\n",
    "axes[1].set_xlabel('Epoca')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "for ep in [5, 10, 20]:\n",
    "    axes[1].axvline(x=ep, color='gray', linestyle=':', alpha=0.5)\n",
    "\n",
    "plt.suptitle('Experimento 4: Efecto del Numero de Epocas', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar accuracy y loss en puntos clave\n",
    "print(\"=\"*70)\n",
    "print(\"RENDIMIENTO EN DIFERENTES EPOCAS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Epoca':<10} {'Train Acc':<15} {'Val Acc':<15} {'Train Loss':<15} {'Val Loss':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for ep in [4, 9, 19, 49]:  # Indices 0-based para epocas 5, 10, 20, 50\n",
    "    t_acc = historial_epocas.history['accuracy'][ep]\n",
    "    v_acc = historial_epocas.history['val_accuracy'][ep]\n",
    "    t_loss = historial_epocas.history['loss'][ep]\n",
    "    v_loss = historial_epocas.history['val_loss'][ep]\n",
    "    print(f\"{ep+1:<10} {t_acc:<15.4f} {v_acc:<15.4f} {t_loss:<15.4f} {v_loss:<15.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Detectar posible overfitting\n",
    "# Overfitting: train_acc sube pero val_acc se estanca o baja\n",
    "#              train_loss baja pero val_loss sube\n",
    "val_losses = historial_epocas.history['val_loss']\n",
    "mejor_epoca = np.argmin(val_losses) + 1\n",
    "print(f\"\\nMejor epoca segun val_loss: Epoca {mejor_epoca}\")\n",
    "print(f\"Val Loss en epoca {mejor_epoca}: {val_losses[mejor_epoca-1]:.4f}\")\n",
    "print(f\"Val Loss en epoca 50: {val_losses[-1]:.4f}\")\n",
    "\n",
    "if val_losses[-1] > val_losses[mejor_epoca-1] * 1.05:\n",
    "    print(\"\\n** ALERTA: Se detecta posible overfitting. **\")\n",
    "    print(\"La perdida de validacion aumento despues de su mejor punto.\")\n",
    "else:\n",
    "    print(\"\\nNo se detecta overfitting significativo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta - Experimento 4:\n",
    "\n",
    "**Pregunta:** ?A partir de cuantas epocas el modelo deja de mejorar significativamente? ?Observas overfitting con muchas epocas? ?Como lo sabes? (Pista: observa la diferencia entre las curvas de entrenamiento y validacion).\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experimento 5: Visualizando Predicciones y Matriz de Confusion\n",
    "\n",
    "Los numeros son utiles, pero **ver** lo que el modelo predice nos da una comprension mucho mas profunda de su comportamiento. Vamos a:\n",
    "\n",
    "1. Visualizar predicciones individuales\n",
    "2. Construir una matriz de confusion para ver donde se equivoca sistematicamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 5: Visualizacion de predicciones\n",
    "# ============================================================\n",
    "\n",
    "# Usaremos el mejor modelo del Experimento 2 (modelo_B con 2 capas)\n",
    "# para hacer predicciones sobre el conjunto de prueba.\n",
    "\n",
    "# Obtener predicciones\n",
    "predicciones = modelo_B.predict(x_test, verbose=0)\n",
    "\n",
    "# predicciones tiene forma (10000, 10): 10 probabilidades por imagen\n",
    "# Tomamos la clase con mayor probabilidad\n",
    "clases_predichas = np.argmax(predicciones, axis=1)\n",
    "clases_reales = y_test_raw  # Las etiquetas originales (no one-hot)\n",
    "\n",
    "print(f\"Forma de las predicciones: {predicciones.shape}\")\n",
    "print(f\"\\nEjemplo de prediccion para la primera imagen:\")\n",
    "print(f\"  Probabilidades: {np.round(predicciones[0], 4)}\")\n",
    "print(f\"  Clase predicha: {clases_predichas[0]}\")\n",
    "print(f\"  Clase real:     {clases_reales[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualizar 25 predicciones ---\n",
    "# Verde = prediccion correcta, Rojo = prediccion incorrecta\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(14, 14))\n",
    "fig.suptitle('Predicciones del Modelo (Verde=Correcta, Rojo=Incorrecta)',\n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Seleccionar 25 indices aleatorios\n",
    "indices = np.random.choice(len(x_test), 25, replace=False)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = indices[i]\n",
    "    imagen = x_test_raw[idx]  # Imagen original sin normalizar para visualizacion\n",
    "    pred = clases_predichas[idx]\n",
    "    real = clases_reales[idx]\n",
    "    confianza = predicciones[idx][pred] * 100  # Probabilidad de la prediccion\n",
    "    \n",
    "    ax.imshow(imagen, cmap='gray')\n",
    "    \n",
    "    # Color del titulo segun si es correcta o no\n",
    "    if pred == real:\n",
    "        color = 'green'\n",
    "        titulo = f'Pred: {pred} (Real: {real})\\nConf: {confianza:.1f}%'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        titulo = f'Pred: {pred} (Real: {real})\\nConf: {confianza:.1f}%'\n",
    "    \n",
    "    ax.set_title(titulo, fontsize=10, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Matriz de Confusion ---\n",
    "# La matriz de confusion muestra cuantas veces cada clase fue predicha\n",
    "# como cada otra clase. Es una herramienta fundamental para entender\n",
    "# los errores del modelo.\n",
    "\n",
    "cm = confusion_matrix(clases_reales, clases_predichas)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=range(10), yticklabels=range(10),\n",
    "            linewidths=0.5, linecolor='gray')\n",
    "plt.title('Matriz de Confusion - MNIST', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Clase Predicha', fontsize=14)\n",
    "plt.ylabel('Clase Real', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretacion:\n",
    "# - Los valores en la diagonal son las predicciones correctas\n",
    "# - Los valores fuera de la diagonal son los errores\n",
    "# - Fila i, Columna j: cuantas veces el digito i fue predicho como j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reporte de clasificacion ---\n",
    "# Muestra precision, recall y f1-score por cada clase\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REPORTE DE CLASIFICACION DETALLADO\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(clases_reales, clases_predichas,\n",
    "                            target_names=[f'Digito {i}' for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Encontrar los errores mas comunes ---\n",
    "# Veamos cuales son las confusiones mas frecuentes\n",
    "\n",
    "# Poner la diagonal en 0 para ver solo errores\n",
    "cm_errores = cm.copy()\n",
    "np.fill_diagonal(cm_errores, 0)\n",
    "\n",
    "# Encontrar las 5 confusiones mas comunes\n",
    "print(\"\\nTOP 5 CONFUSIONES MAS FRECUENTES:\")\n",
    "print(\"-\"*40)\n",
    "for _ in range(5):\n",
    "    i, j = np.unravel_index(cm_errores.argmax(), cm_errores.shape)\n",
    "    print(f\"  Digito {i} confundido con {j}: {cm_errores[i, j]} veces\")\n",
    "    cm_errores[i, j] = 0  # Poner en 0 para encontrar el siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta - Experimento 5:\n",
    "\n",
    "**Pregunta:** ?Que digitos confunde mas el modelo entre si? ?Tiene sentido que esos digitos se confundan? ?Por que? (Piensa en como lucen visualmente esos digitos.)\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experimento 6: API Functional de Keras (Introduccion)\n",
    "\n",
    "Hasta ahora hemos usado la **API Sequential**, que apila capas una tras otra como una torre de bloques. Pero Keras tambien ofrece la **API Functional**, que es mas flexible y permite crear arquitecturas mas complejas.\n",
    "\n",
    "**?Cuando usar cada una?**\n",
    "\n",
    "- **Sequential:** Cuando tu modelo es una secuencia lineal de capas (la salida de una es la entrada de la siguiente). Perfecto para empezar.\n",
    "- **Functional:** Cuando necesitas:\n",
    "  - Multiples entradas o salidas\n",
    "  - Conexiones residuales (skip connections)\n",
    "  - Arquitecturas en paralelo (como redes siamesas)\n",
    "  - Compartir capas entre diferentes partes del modelo\n",
    "\n",
    "Vamos a reconstruir nuestro mejor modelo usando la API Functional para entender la diferencia en sintaxis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EXPERIMENTO 6: API Functional de Keras\n",
    "# ============================================================\n",
    "\n",
    "# --- API Sequential (lo que ya conocemos) ---\n",
    "# modelo_seq = keras.Sequential([\n",
    "#     layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# --- API Functional (nueva forma) ---\n",
    "# En la API Functional, definimos cada capa como una funcion\n",
    "# que toma como entrada la salida de la capa anterior.\n",
    "\n",
    "# Paso 1: Definir la entrada\n",
    "entrada = keras.Input(shape=(784,), name='entrada')\n",
    "\n",
    "# Paso 2: Definir las capas ocultas como funciones\n",
    "# Nota: cada capa recibe la salida de la anterior como argumento\n",
    "x = layers.Dense(128, activation='relu', name='capa_oculta_1')(entrada)\n",
    "x = layers.Dense(64, activation='relu', name='capa_oculta_2')(x)\n",
    "\n",
    "# Paso 3: Definir la capa de salida\n",
    "salida = layers.Dense(10, activation='softmax', name='capa_salida')(x)\n",
    "\n",
    "# Paso 4: Crear el modelo especificando entrada y salida\n",
    "modelo_functional = keras.Model(inputs=entrada, outputs=salida, name='Modelo_Functional')\n",
    "\n",
    "# Ver el resumen\n",
    "modelo_functional.summary()\n",
    "\n",
    "print(\"\\nEl modelo se ve identico al Sequential, pero la API Functional\")\n",
    "print(\"permite crear arquitecturas mucho mas complejas en el futuro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar y entrenar el modelo Functional\n",
    "modelo_functional.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Entrenando modelo con API Functional...\")\n",
    "historial_func = modelo_functional.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluar\n",
    "loss_func, acc_func = modelo_functional.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nAccuracy en test: {acc_func:.4f} ({acc_func*100:.2f}%)\")\n",
    "print(\"\\nEl resultado es similar al modelo Sequential, como esperabamos.\")\n",
    "print(\"La diferencia esta en la flexibilidad, no en el rendimiento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ejemplo avanzado: Modelo con multiples salidas (solo demostrativo) ---\n",
    "# Este ejemplo muestra por que la API Functional es poderosa.\n",
    "# Imaginemos que queremos predecir:\n",
    "#   1. El digito (0-9)\n",
    "#   2. Si el digito es par o impar (binario)\n",
    "\n",
    "# Con Sequential esto NO es posible. Con Functional SI:\n",
    "\n",
    "entrada_demo = keras.Input(shape=(784,), name='entrada_demo')\n",
    "x_demo = layers.Dense(128, activation='relu', name='compartida_1')(entrada_demo)\n",
    "x_demo = layers.Dense(64, activation='relu', name='compartida_2')(x_demo)\n",
    "\n",
    "# Salida 1: Clasificacion del digito (10 clases)\n",
    "salida_digito = layers.Dense(10, activation='softmax', name='salida_digito')(x_demo)\n",
    "\n",
    "# Salida 2: Par o impar (2 clases)\n",
    "salida_paridad = layers.Dense(2, activation='softmax', name='salida_paridad')(x_demo)\n",
    "\n",
    "modelo_multi = keras.Model(\n",
    "    inputs=entrada_demo,\n",
    "    outputs=[salida_digito, salida_paridad],\n",
    "    name='Modelo_MultiSalida'\n",
    ")\n",
    "\n",
    "modelo_multi.summary()\n",
    "\n",
    "print(\"\\nEste modelo tiene DOS salidas! Esto es imposible con Sequential.\")\n",
    "print(\"En guias futuras veremos arquitecturas mas complejas usando la API Functional.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Tu respuesta - Experimento 6:\n",
    "\n",
    "**Pregunta:** ?Que ventajas tiene la API Functional sobre la Sequential? ?En que casos usarias cada una?\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Sintesis y Reflexion Final\n",
    "\n",
    "Has completado una serie de experimentos fundamentales sobre redes neuronales. Ahora es momento de sintetizar lo que aprendiste.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta de Sintesis 1:\n",
    "\n",
    "**?Cuales son los hiperparametros mas importantes que observaste al construir una red neuronal?** Menciona al menos 4 y explica como afecta cada uno al rendimiento.\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta de Sintesis 2:\n",
    "\n",
    "**Compara el perceptron de la Guia 01 con la red neuronal de esta guia.** ?Que problema resolvio agregar capas? ?Que diferencias fundamentales hay entre un perceptron simple y un MLP?\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\u270d\\ufe0f Pregunta de Sintesis 3:\n",
    "\n",
    "**Si tuvieras que clasificar imagenes mas complejas (por ejemplo, fotos de objetos reales como perros, gatos, carros), ?crees que esta red (MLP densa) funcionaria bien? ?Por que?** (Pista: piensa en el preprocesamiento que hicimos -- aplanar la imagen a un vector -- y que informacion se pierde.)\n",
    "\n",
    "*Escribe aqui tu respuesta...*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Reto Extra: Fashion-MNIST\n",
    "\n",
    "Si llegaste hasta aqui y quieres un desafio adicional, prueba tus habilidades con un dataset un poco mas dificil: **Fashion-MNIST**.\n",
    "\n",
    "Fashion-MNIST tiene la misma estructura que MNIST (60,000 imagenes de 28x28 en escala de grises, 10 clases), pero en lugar de digitos, contiene imagenes de prendas de ropa:\n",
    "\n",
    "| Clase | Descripcion |\n",
    "|:-----:|:-----------:|\n",
    "| 0 | Camiseta/Top |\n",
    "| 1 | Pantalon |\n",
    "| 2 | Jersey/Sueter |\n",
    "| 3 | Vestido |\n",
    "| 4 | Abrigo |\n",
    "| 5 | Sandalia |\n",
    "| 6 | Camisa |\n",
    "| 7 | Zapatilla deportiva |\n",
    "| 8 | Bolso |\n",
    "| 9 | Bota |\n",
    "\n",
    "### Instrucciones:\n",
    "\n",
    "1. Carga el dataset con `keras.datasets.fashion_mnist.load_data()`\n",
    "2. Realiza el mismo preprocesamiento que hicimos con MNIST (normalizar, reshape, one-hot)\n",
    "3. Disena tu propia arquitectura de red. Tu decides cuantas capas y neuronas usar.\n",
    "4. Entrena, evalua y muestra la matriz de confusion.\n",
    "5. **Objetivo:** Alcanzar al menos un **88% de accuracy** en el conjunto de prueba.\n",
    "\n",
    "### Pistas:\n",
    "- Fashion-MNIST es mas dificil que MNIST. Los digitos tienen formas mas simples que la ropa.\n",
    "- Puedes necesitar una red un poco mas grande o mas epocas.\n",
    "- Presta atencion al overfitting: Fashion-MNIST es mas propenso a ello.\n",
    "- Experimenta con diferentes configuraciones antes de elegir tu arquitectura final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RETO EXTRA: Fashion-MNIST\n",
    "# ============================================================\n",
    "\n",
    "# Nombres de las clases de Fashion-MNIST (para visualizacion)\n",
    "nombres_clases_fashion = [\n",
    "    'Camiseta', 'Pantalon', 'Jersey', 'Vestido', 'Abrigo',\n",
    "    'Sandalia', 'Camisa', 'Zapatilla', 'Bolso', 'Bota'\n",
    "]\n",
    "\n",
    "# --- Paso 1: Cargar los datos ---\n",
    "(x_train_f, y_train_f), (x_test_f, y_test_f) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "print(f\"Fashion-MNIST cargado!\")\n",
    "print(f\"Entrenamiento: {x_train_f.shape}\")\n",
    "print(f\"Prueba: {x_test_f.shape}\")\n",
    "\n",
    "# Visualizar algunos ejemplos\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "fig.suptitle('Ejemplos de Fashion-MNIST', fontsize=16, fontweight='bold')\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.random.randint(0, len(x_train_f))\n",
    "    ax.imshow(x_train_f[idx], cmap='gray')\n",
    "    ax.set_title(nombres_clases_fashion[y_train_f[idx]], fontsize=11)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 2: Preprocesamiento ---\n",
    "# Escribe tu codigo de preprocesamiento aqui.\n",
    "# Recuerda: normalizar, reshape y one-hot encoding.\n",
    "\n",
    "# Tu codigo aqui:\n",
    "# x_train_fashion = ...\n",
    "# x_test_fashion = ...\n",
    "# y_train_fashion = ...\n",
    "# y_test_fashion = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 3: Disenar tu red neuronal ---\n",
    "# Crea tu propia arquitectura. Experimenta!\n",
    "# Objetivo: al menos 88% de accuracy en test.\n",
    "\n",
    "# Tu codigo aqui:\n",
    "# modelo_fashion = keras.Sequential([...])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 4: Compilar y entrenar ---\n",
    "\n",
    "# Tu codigo aqui:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paso 5: Evaluar y mostrar resultados ---\n",
    "# Incluye:\n",
    "#   - Accuracy en test\n",
    "#   - Curvas de entrenamiento\n",
    "#   - Matriz de confusion\n",
    "\n",
    "# Tu codigo aqui:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Referencias\n",
    "\n",
    "### Documentacion oficial\n",
    "- **TensorFlow/Keras:** [https://www.tensorflow.org/guide/keras](https://www.tensorflow.org/guide/keras)\n",
    "- **API Sequential:** [https://keras.io/guides/sequential_model/](https://keras.io/guides/sequential_model/)\n",
    "- **API Functional:** [https://keras.io/guides/functional_api/](https://keras.io/guides/functional_api/)\n",
    "\n",
    "### Tutoriales recomendados\n",
    "- **Tutorial oficial de MNIST con Keras:** [https://www.tensorflow.org/tutorials/quickstart/beginner](https://www.tensorflow.org/tutorials/quickstart/beginner)\n",
    "- **Fashion-MNIST:** [https://www.tensorflow.org/tutorials/keras/classification](https://www.tensorflow.org/tutorials/keras/classification)\n",
    "\n",
    "### Videos recomendados\n",
    "- **3Blue1Brown - Neural Networks (serie completa):** [https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "  - Capitulo 1: Que es una red neuronal\n",
    "  - Capitulo 2: Descenso de gradiente\n",
    "  - Capitulo 3: Backpropagation\n",
    "\n",
    "### Lecturas adicionales\n",
    "- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press. Capitulos 6-8.\n",
    "- LeCun, Y., Cortes, C., & Burges, C. J. (2010). MNIST handwritten digit database.\n",
    "\n",
    "---\n",
    "\n",
    "**Proxima guia:** En la Guia 03 exploraremos tecnicas de **regularizacion** (Dropout, L2, Early Stopping) para combatir el overfitting que observamos en esta guia, y aprenderemos sobre **Batch Normalization**.\n",
    "\n",
    "---\n",
    "*Electiva II - Deep Learning | Tecnologico de Antioquia | 2026-1*"
   ]
  }
 ]
}